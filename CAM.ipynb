{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e285b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1d87c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=56\n",
    "W=56\n",
    "C=64\n",
    "B=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0138553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_list=[]\n",
    "# in_x1 = torch.rand(B,C,H,W)\n",
    "# in_x2 = torch.rand(B,C,H,W)\n",
    "# in_x3 = torch.rand(B,C,H,W)\n",
    "# in_x4 = torch.rand(B,C,H,W)\n",
    "\n",
    "# x_list.append(in_x1)\n",
    "# x_list.append(in_x2)\n",
    "# x_list.append(in_x3)\n",
    "# x_list.append(in_x4)\n",
    "# # print(x_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8bc7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Conv3d_BN_concat(nn.Module):\n",
    "#     # input: attention list of length #path\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         in_ch,\n",
    "#         out_ch,\n",
    "#         kernel_size=1,\n",
    "#         stride=1,\n",
    "#         pad=0,\n",
    "#         dilation=1,\n",
    "#         groups=1,\n",
    "#         bn_weight_init=1,\n",
    "#         act_layer=None,\n",
    "#         norm_cfg=\"BN\",\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "    \n",
    "#         self.bn = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "#         torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "#         torch.nn.init.constant_(self.bn.bias, 0)\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.xavier_uniform_(m.weight)\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "       \n",
    "#         self.interact_concat = nn.Sequential(\n",
    "#             nn.Conv3d(in_ch, out_ch, kernel_size=(4,1,1)),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_list):\n",
    "        \n",
    "#         b,c,h,w = x_list[0].shape\n",
    "#         print(f\"b:{b} c:{c} h:{h} w:{w}\")\n",
    "#         out_3d = []\n",
    "#         for ip in range(len(x_list)):\n",
    "# #             print(f'ip:{ip}')\n",
    "#             in_x = x_list[ip]  \n",
    "# #             print(f'in each {ip} iteration: {in_x.shape}')\n",
    "#             in_x = in_x.unsqueeze_(dim=2)\n",
    "#             out_3d.append(in_x)\n",
    "# #             print(f'{ip} path extend to shape: {in_x.shape}')\n",
    "\n",
    "#         x = torch.cat(out_3d, dim=2)\n",
    "# #         print(f'after concat: {x.shape}')\n",
    "#         x = torch.squeeze(self.interact_concat(x), dim=2)\n",
    "# #         print(f'after squeeze: {x.shape}')\n",
    "#         x = self.bn(x)\n",
    "\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48923c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:4 c:64 h:56 w:56\n",
      "torch.Size([4, 128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# conv3d_concat = Conv3d_BN_concat(C,2*C)\n",
    "# x_out = conv3d_concat(x_list)\n",
    "# print(x_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5933026",
   "metadata": {},
   "source": [
    "https://github.com/VITA-Group/ABD-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb3460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e6c7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv2d, Parameter, Softmax\n",
    "\n",
    "class CAM_Module(Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.channel_in = in_dim\n",
    "\n",
    "        self.gamma = Parameter(torch.zeros(1))\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "        self.numpath = 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B  C  H  W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C,numpath, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, numpath, -1) #b c 4 n\n",
    "        print(f'Shape of query: {proj_query.shape}')\n",
    "        proj_key = x.view(m_batchsize, C,numpath, -1).permute(0, 1, 3, 2) #b c n 4\n",
    "        print(f'Shape of key: {proj_key.shape}') \n",
    "        energy = torch.matmul(proj_query, proj_key) # b c 4 4\n",
    "        print(f'Shape of energy: {energy.shape}')\n",
    "        max_energy_0 = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)\n",
    "        energy_new = max_energy_0 - energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        print(f'Shape of attention: {attention.shape}')\n",
    "        proj_value = x.view(m_batchsize, C, numpath, -1)\n",
    "        print(f'Shape of proj_value: {proj_value.shape}')\n",
    "\n",
    "        out = torch.matmul(attention, proj_value)# can be replace by torch.matmul\n",
    "        out = out.view(m_batchsize, C, numpath, height, width)\n",
    "\n",
    "#         logging.debug('cam device: {}, {}'.format(out.device, self.gamma.device))\n",
    "        gamma = self.gamma.to(out.device)\n",
    "        out = gamma * out + x\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d83f6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = CAM_Module(in_dim = C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2687709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 4, 56, 56])\n",
      "Shape of query: torch.Size([4, 64, 4, 3136])\n",
      "Shape of key: torch.Size([4, 64, 3136, 4])\n",
      "Shape of energy: torch.Size([4, 64, 4, 4])\n",
      "Shape of attention: torch.Size([4, 64, 4, 4])\n",
      "Shape of proj_value: torch.Size([4, 64, 4, 3136])\n",
      "torch.Size([4, 64, 4, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c600c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3d_BN_channel_attention_concat(nn.Module):\n",
    "    # input: attention list of length #path\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch,\n",
    "        out_ch,\n",
    "        kernel_size=1,\n",
    "        stride=1,\n",
    "        pad=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bn_weight_init=1,\n",
    "        act_layer=None,\n",
    "        norm_cfg=\"BN\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "       \n",
    "        self.interact_concat = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=(4,1,1)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.channelAttention = CAM_Module(in_dim = in_ch)\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        \n",
    "        b,c,h,w = x_list[0].shape\n",
    "        print(f\"b:{b} c:{c} h:{h} w:{w}\")\n",
    "        out_3d = []\n",
    "        for ip in range(len(x_list)):\n",
    "#             print(f'ip:{ip}')\n",
    "            in_x = x_list[ip]  \n",
    "#             print(f'in each {ip} iteration: {in_x.shape}')\n",
    "            in_x = in_x.unsqueeze_(dim=2)\n",
    "            out_3d.append(in_x)\n",
    "#             print(f'{ip} path extend to shape: {in_x.shape}')\n",
    "\n",
    "        x = torch.cat(out_3d, dim=2)\n",
    "        print(f'before channel attention: {x.shape}')\n",
    "        x = self.channelAttention(x)\n",
    "        print(f'after concat: {x.shape}')\n",
    "        x = torch.squeeze(self.interact_concat(x), dim=2)\n",
    "        print(f'after squeeze: {x.shape}')\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c33a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list=[]\n",
    "in_x1 = torch.rand(B,C,H,W)\n",
    "in_x2 = torch.rand(B,C,H,W)\n",
    "in_x3 = torch.rand(B,C,H,W)\n",
    "in_x4 = torch.rand(B,C,H,W)\n",
    "\n",
    "x_list.append(in_x1)\n",
    "x_list.append(in_x2)\n",
    "x_list.append(in_x3)\n",
    "x_list.append(in_x4)\n",
    "\n",
    "attention_concat = Conv3d_BN_channel_attention_concat(C,2*C)\n",
    "x_out = attention_concat(x_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
