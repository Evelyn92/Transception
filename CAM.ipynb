{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95d8f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b92a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=56\n",
    "W=56\n",
    "C=64\n",
    "B=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "509f306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_list=[]\n",
    "# in_x1 = torch.rand(B,C,H,W)\n",
    "# in_x2 = torch.rand(B,C,H,W)\n",
    "# in_x3 = torch.rand(B,C,H,W)\n",
    "# in_x4 = torch.rand(B,C,H,W)\n",
    "\n",
    "# x_list.append(in_x1)\n",
    "# x_list.append(in_x2)\n",
    "# x_list.append(in_x3)\n",
    "# x_list.append(in_x4)\n",
    "# # print(x_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32f57745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Conv3d_BN_concat(nn.Module):\n",
    "#     # input: attention list of length #path\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         in_ch,\n",
    "#         out_ch,\n",
    "#         kernel_size=1,\n",
    "#         stride=1,\n",
    "#         pad=0,\n",
    "#         dilation=1,\n",
    "#         groups=1,\n",
    "#         bn_weight_init=1,\n",
    "#         act_layer=None,\n",
    "#         norm_cfg=\"BN\",\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "    \n",
    "#         self.bn = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "#         torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "#         torch.nn.init.constant_(self.bn.bias, 0)\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.xavier_uniform_(m.weight)\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "       \n",
    "#         self.interact_concat = nn.Sequential(\n",
    "#             nn.Conv3d(in_ch, out_ch, kernel_size=(4,1,1)),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_list):\n",
    "        \n",
    "#         b,c,h,w = x_list[0].shape\n",
    "#         print(f\"b:{b} c:{c} h:{h} w:{w}\")\n",
    "#         out_3d = []\n",
    "#         for ip in range(len(x_list)):\n",
    "# #             print(f'ip:{ip}')\n",
    "#             in_x = x_list[ip]  \n",
    "# #             print(f'in each {ip} iteration: {in_x.shape}')\n",
    "#             in_x = in_x.unsqueeze_(dim=2)\n",
    "#             out_3d.append(in_x)\n",
    "# #             print(f'{ip} path extend to shape: {in_x.shape}')\n",
    "\n",
    "#         x = torch.cat(out_3d, dim=2)\n",
    "# #         print(f'after concat: {x.shape}')\n",
    "#         x = torch.squeeze(self.interact_concat(x), dim=2)\n",
    "# #         print(f'after squeeze: {x.shape}')\n",
    "#         x = self.bn(x)\n",
    "\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "684623ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:4 c:64 h:56 w:56\n",
      "torch.Size([4, 128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# conv3d_concat = Conv3d_BN_concat(C,2*C)\n",
    "# x_out = conv3d_concat(x_list)\n",
    "# print(x_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c8eed",
   "metadata": {},
   "source": [
    "https://github.com/VITA-Group/ABD-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b111f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e7d876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv2d, Parameter, Softmax\n",
    "\n",
    "class CAM_Module(Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.channel_in = in_dim\n",
    "\n",
    "        self.gamma = Parameter(torch.zeros(1))\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "        self.numpath = 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B  C  H  W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C,numpath, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, numpath, -1) #b c 4 n\n",
    "        print(f'Shape of query: {proj_query.shape}')\n",
    "        proj_key = x.view(m_batchsize, C,numpath, -1).permute(0, 1, 3, 2) #b c n 4\n",
    "        print(f'Shape of key: {proj_key.shape}') \n",
    "        energy = torch.matmul(proj_query, proj_key) # b c 4 4\n",
    "        print(f'Shape of energy: {energy.shape}')\n",
    "        max_energy_0 = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)\n",
    "        energy_new = max_energy_0 - energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        print(f'Shape of attention: {attention.shape}')\n",
    "        proj_value = x.view(m_batchsize, C, numpath, -1)\n",
    "        print(f'Shape of proj_value: {proj_value.shape}')\n",
    "\n",
    "        out = torch.matmul(attention, proj_value)# can be replace by torch.matmul\n",
    "        out = out.view(m_batchsize, C, numpath, height, width)\n",
    "\n",
    "#         logging.debug('cam device: {}, {}'.format(out.device, self.gamma.device))\n",
    "        gamma = self.gamma.to(out.device)\n",
    "        out = gamma * out + x\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7461ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = CAM_Module(in_dim = C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d04dceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 4, 56, 56])\n",
      "Shape of query: torch.Size([4, 64, 4, 3136])\n",
      "Shape of key: torch.Size([4, 64, 3136, 4])\n",
      "Shape of energy: torch.Size([4, 64, 4, 4])\n",
      "Shape of attention: torch.Size([4, 64, 4, 4])\n",
      "Shape of proj_value: torch.Size([4, 64, 4, 3136])\n",
      "torch.Size([4, 64, 4, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8f828d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3d_BN_channel_attention_concat(nn.Module):\n",
    "    # input: attention list of length #path\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch,\n",
    "        out_ch,\n",
    "        kernel_size=1,\n",
    "        stride=1,\n",
    "        pad=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bn_weight_init=1,\n",
    "        act_layer=None,\n",
    "        norm_cfg=\"BN\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "       \n",
    "        self.interact_concat = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=(4,1,1)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.channelAttention = CAM_Module(in_dim = in_ch)\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        \n",
    "        b,c,h,w = x_list[0].shape\n",
    "        print(f\"b:{b} c:{c} h:{h} w:{w}\")\n",
    "        out_3d = []\n",
    "        for ip in range(len(x_list)):\n",
    "#             print(f'ip:{ip}')\n",
    "            in_x = x_list[ip]  \n",
    "#             print(f'in each {ip} iteration: {in_x.shape}')\n",
    "            in_x = in_x.unsqueeze_(dim=2)\n",
    "            out_3d.append(in_x)\n",
    "#             print(f'{ip} path extend to shape: {in_x.shape}')\n",
    "\n",
    "        x = torch.cat(out_3d, dim=2)\n",
    "        print(f'before channel attention: {x.shape}')\n",
    "        x = self.channelAttention(x)\n",
    "        print(f'after concat: {x.shape}')\n",
    "        x = torch.squeeze(self.interact_concat(x), dim=2)\n",
    "        print(f'after squeeze: {x.shape}')\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "408a7095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:4 c:64 h:56 w:56\n",
      "before channel attention: torch.Size([4, 64, 4, 56, 56])\n",
      "Shape of query: torch.Size([4, 64, 4, 3136])\n",
      "Shape of key: torch.Size([4, 64, 3136, 4])\n",
      "Shape of energy: torch.Size([4, 64, 4, 4])\n",
      "Shape of attention: torch.Size([4, 64, 4, 4])\n",
      "Shape of proj_value: torch.Size([4, 64, 4, 3136])\n",
      "after concat: torch.Size([4, 64, 4, 56, 56])\n",
      "after squeeze: torch.Size([4, 128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "x_list=[]\n",
    "in_x1 = torch.rand(B,C,H,W)\n",
    "in_x2 = torch.rand(B,C,H,W)\n",
    "in_x3 = torch.rand(B,C,H,W)\n",
    "in_x4 = torch.rand(B,C,H,W)\n",
    "\n",
    "x_list.append(in_x1)\n",
    "x_list.append(in_x2)\n",
    "x_list.append(in_x3)\n",
    "x_list.append(in_x4)\n",
    "\n",
    "attention_concat = Conv3d_BN_channel_attention_concat(C,2*C)\n",
    "x_out = attention_concat(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15d439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv2d, Parameter, Softmax\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8de77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3015, 0.1978, 0.3139, 0.1868],\n",
      "        [0.2168, 0.2544, 0.2007, 0.3281],\n",
      "        [0.3078, 0.2537, 0.1793, 0.2592],\n",
      "        [0.2129, 0.3217, 0.3074, 0.1580]])\n",
      "tensor([[0.2658, 0.1729, 0.2833, 0.1907],\n",
      "        [0.1578, 0.1836, 0.1496, 0.2765],\n",
      "        [0.3231, 0.2641, 0.1928, 0.3151],\n",
      "        [0.2533, 0.3794, 0.3744, 0.2176]])\n",
      "tensor([[0.3015, 0.1978, 0.3139, 0.1868],\n",
      "        [0.2168, 0.2544, 0.2007, 0.3281],\n",
      "        [0.3078, 0.2537, 0.1793, 0.2592],\n",
      "        [0.2129, 0.3217, 0.3074, 0.1580]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "in_x = torch.rand(1,4,4)\n",
    "\n",
    "softmax1 = Softmax(dim=-1)\n",
    "x1 = softmax1(in_x)\n",
    "print(x1[0])\n",
    "\n",
    "softmax2 = Softmax(dim=1)\n",
    "x2 = softmax2(in_x)\n",
    "print(x2[0])\n",
    "\n",
    "softmax3 = Softmax(dim=2)\n",
    "x3 = softmax3(in_x)\n",
    "print(x3[0])\n",
    "\n",
    "softmax4 = Softmax(dim=0)\n",
    "x4 = softmax4(in_x)\n",
    "print(x4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd150b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([[[0.3139],\n",
      "         [0.3281],\n",
      "         [0.3078],\n",
      "         [0.3217]]]),\n",
      "indices=tensor([[[2],\n",
      "         [3],\n",
      "         [0],\n",
      "         [1]]]))\n",
      "tensor([[[0.3139],\n",
      "         [0.3281],\n",
      "         [0.3078],\n",
      "         [0.3217]]])\n"
     ]
    }
   ],
   "source": [
    "max_x3 = torch.max(x3,-1,keepdim=True)\n",
    "print(max_x3)\n",
    "max_x3 = max_x3[0].expand_as(max_x3[0])\n",
    "print(max_x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2bef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2958, 0.2056, 0.2667, 0.2319],\n",
      "        [0.2414, 0.2539, 0.2209, 0.2838],\n",
      "        [0.2664, 0.3006, 0.1785, 0.2545],\n",
      "        [0.1799, 0.2220, 0.2343, 0.3638]])\n"
     ]
    }
   ],
   "source": [
    "in_x = torch.rand(1,64, 4,4)\n",
    "\n",
    "softmax1 = Softmax(dim=-1)\n",
    "x5 = softmax1(in_x)\n",
    "print(x5[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28ebae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
