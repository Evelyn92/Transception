{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7416f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from networks.segformer import *\n",
    "# For jupyter notebook below\n",
    "from Transception import *\n",
    "from EffSegformer import *\n",
    "from typing import Tuple\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450a4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MiT(nn.Module):\n",
    "#     def __init__(self, image_size, in_dim, key_dim, value_dim, layers, head_count=1, token_mlp='mix_skip'):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.Hs=[56, 28, 14, 7]\n",
    "#         self.Ws=[56, 28, 14, 7]\n",
    "\n",
    "#         patch_sizes = [7, 3, 3, 3]\n",
    "#         patch_sizes1 = [7, 3, 3, 3]\n",
    "#         patch_sizes2 = [5, 1, 1, 1]\n",
    "\n",
    "#         strides = [4, 2, 2, 2]\n",
    "#         # padding_sizes = [3, 1, 1, 1]\n",
    "#         dil_padding_sizes1 = [3, 0, 0, 0]\n",
    "#         dil_padding_sizes2 = [3, 0, 0, 0]\\\n",
    "\n",
    "#         # 1 by 1 convolution to alter the dimension\n",
    "#         self.conv1_1_s1 = nn.Conv2d(2*in_dim[0], in_dim[0], 1)\n",
    "#         self.conv1_1_s2 = nn.Conv2d(2*in_dim[1], in_dim[1], 1)\n",
    "#         self.conv1_1_s3 = nn.Conv2d(2*in_dim[2], in_dim[2], 1)\n",
    "#         self.conv1_1_s4 = nn.Conv2d(2*in_dim[3], in_dim[3], 1)\n",
    "\n",
    "#         # patch_embed\n",
    "#         # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "#         self.patch_embed1_1 = OverlapPatchEmbeddings_fuse(image_size, patch_sizes1[0], strides[0], dil_padding_sizes1[0], 3, in_dim[0])\n",
    "#         self.patch_embed1_2 = OverlapPatchEmbeddings_fuse(image_size, patch_sizes2[0], strides[0], dil_padding_sizes2[0], 3, in_dim[0])\n",
    "\n",
    "#         self.patch_embed2_1 = OverlapPatchEmbeddings_fuse(image_size//4, patch_sizes1[1], strides[1], dil_padding_sizes1[1],in_dim[0], in_dim[1])\n",
    "#         self.patch_embed2_2 = OverlapPatchEmbeddings_fuse(image_size//4, patch_sizes2[1], strides[1], dil_padding_sizes2[1],in_dim[0], in_dim[1])\n",
    "\n",
    "#         self.patch_embed3_1 = OverlapPatchEmbeddings_fuse(image_size//8, patch_sizes1[2], strides[2], dil_padding_sizes1[2],in_dim[1], in_dim[2])\n",
    "#         self.patch_embed3_2 = OverlapPatchEmbeddings_fuse(image_size//8, patch_sizes2[2], strides[2], dil_padding_sizes2[2],in_dim[1], in_dim[2])\n",
    "\n",
    "#         self.patch_embed4_1 = OverlapPatchEmbeddings_fuse(image_size//16, patch_sizes1[3], strides[3], dil_padding_sizes1[3],in_dim[2], in_dim[3])\n",
    "#         self.patch_embed4_2 = OverlapPatchEmbeddings_fuse(image_size//16, patch_sizes2[3], strides[3], dil_padding_sizes2[3],in_dim[2], in_dim[3])\n",
    "        \n",
    "#         # transformer encoder\n",
    "#         self.block1 = nn.ModuleList([ \n",
    "#             EfficientTransformerBlockFuse(in_dim[0], key_dim[0], value_dim[0], head_count, token_mlp)\n",
    "#         for _ in range(layers[0])])\n",
    "#         self.norm1 = nn.LayerNorm(in_dim[0])\n",
    "\n",
    "#         self.block2 = nn.ModuleList([\n",
    "#             EfficientTransformerBlockFuse(in_dim[1], key_dim[1], value_dim[1], head_count, token_mlp)\n",
    "#         for _ in range(layers[1])])\n",
    "#         self.norm2 = nn.LayerNorm(in_dim[1])\n",
    "\n",
    "#         self.block3 = nn.ModuleList([\n",
    "#             EfficientTransformerBlockFuse(in_dim[2], key_dim[2], value_dim[2], head_count, token_mlp)\n",
    "#         for _ in range(layers[2])])\n",
    "#         self.norm3 = nn.LayerNorm(in_dim[2])\n",
    "\n",
    "#         self.block4 = nn.ModuleList([\n",
    "#             EfficientTransformerBlockFuse(in_dim[3], key_dim[3], value_dim[3], head_count, token_mlp)\n",
    "#         for _ in range(layers[3])])\n",
    "#         self.norm4 = nn.LayerNorm(in_dim[3])\n",
    "        \n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         B = x.shape[0]\n",
    "#         outs = []\n",
    "\n",
    "#         # stage 1\n",
    "#         x1, H1, W1 = self.patch_embed1_1(x)\n",
    "#         _, nfx1_len, _ = x1.shape\n",
    "#         x2, H2, W2 = self.patch_embed1_2(x)\n",
    "#         _, nfx2_len, _ = x2.shape\n",
    "#         nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "\n",
    "#         for blk in self.block1:\n",
    "#             nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "#         nfx_cat = self.norm1(nfx_cat)\n",
    "#         mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "#         mx2 = nfx_cat[:, nfx1_len: :]\n",
    "#         map_mx1 = mx1.reshape(1,H1,W1,-1)\n",
    "#         map_mx2 = mx2.reshape(1,H2,W2,-1)\n",
    "#         map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "#         map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "        \n",
    "#         map_mx1 = F.interpolate(map_mx1,[self.Hs[0], self.Ws[0]])\n",
    "#         cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "#         x = self.conv1_1_s1(cat_maps)\n",
    "#         # x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "#         outs.append(x)\n",
    "\n",
    "#         # stage 2\n",
    "#         x1, H1, W1 = self.patch_embed2_1(x)\n",
    "#         _, nfx1_len, _ = x1.shape\n",
    "#         x2, H2, W2 = self.patch_embed2_2(x)\n",
    "#         _, nfx2_len, _ = x2.shape\n",
    "#         nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "#         for blk in self.block2:\n",
    "#             nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "#         nfx_cat = self.norm2(nfx_cat)\n",
    "#         mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "#         mx2 = nfx_cat[:, nfx1_len: :]\n",
    "#         map_mx1 = mx1.view(1,H1,W1,-1)\n",
    "#         map_mx2 = mx2.view(1,H2,W2,-1)\n",
    "#         map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "#         map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "#         map_mx1 = F.interpolate(map_mx1,[self.Hs[1], self.Ws[1]])\n",
    "#         cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "#         x = self.conv1_1_s2(cat_maps)\n",
    "#         outs.append(x)\n",
    "\n",
    "#         # stage 3\n",
    "#         x1, H1, W1 = self.patch_embed3_1(x)\n",
    "#         _, nfx1_len, _ = x1.shape\n",
    "#         x2, H2, W2 = self.patch_embed3_2(x)\n",
    "#         _, nfx2_len, _ = x2.shape\n",
    "#         nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "#         for blk in self.block3:\n",
    "#             nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "#         nfx_cat = self.norm3(nfx_cat)\n",
    "#         mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "#         mx2 = nfx_cat[:, nfx1_len: :]\n",
    "#         map_mx1 = mx1.view(1,H1,W1,-1)\n",
    "#         map_mx2 = mx2.view(1,H2,W2,-1)\n",
    "#         map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "#         map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "#         map_mx1 = F.interpolate(map_mx1,[self.Hs[2], self.Ws[2]])\n",
    "#         cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "#         x = self.conv1_1_s3(cat_maps)\n",
    "#         outs.append(x)\n",
    "\n",
    "#         # stage 4\n",
    "#         x1, H1, W1 = self.patch_embed4_1(x)\n",
    "#         _, nfx1_len, _ = x1.shape\n",
    "#         x2, H2, W2 = self.patch_embed4_2(x)\n",
    "#         _, nfx2_len, _ = x2.shape\n",
    "#         nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "#         for blk in self.block4:\n",
    "#             nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "#         nfx_cat = self.norm4(nfx_cat)\n",
    "#         mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "#         mx2 = nfx_cat[:, nfx1_len: :]\n",
    "#         map_mx1 = mx1.view(1,H1,W1,-1)\n",
    "#         map_mx2 = mx2.view(1,H2,W2,-1)\n",
    "#         map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "#         map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "#         map_mx1 = F.interpolate(map_mx1,[self.Hs[3], self.Ws[3]])\n",
    "#         cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "#         x = self.conv1_1_s4(cat_maps)\n",
    "#         outs.append(x)\n",
    "\n",
    "#         return outs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d65b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiT_3inception(nn.Module):\n",
    "    def __init__(self, image_size, in_dim, key_dim, value_dim, layers, head_count=1, token_mlp='mix_skip'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Hs=[56, 28, 14, 7]\n",
    "        self.Ws=[56, 28, 14, 7]\n",
    "\n",
    "        patch_sizes = [7, 3, 3, 3]\n",
    "        patch_sizes1 = [7, 3, 3, 3]\n",
    "        patch_sizes2 = [5, 1, 1, 1]\n",
    "\n",
    "        strides = [4, 2, 2, 2]\n",
    "        padding_sizes = [3, 1, 1, 1]\n",
    "        dil_padding_sizes1 = [3, 0, 0, 0]\n",
    "        dil_padding_sizes2 = [3, 0, 0, 0]\n",
    "\n",
    "        # 1 by 1 convolution to alter the dimension\n",
    "        self.conv1_1_s1 = nn.Conv2d(2*in_dim[0], in_dim[0], 1)\n",
    "        self.conv1_1_s2 = nn.Conv2d(2*in_dim[1], in_dim[1], 1)\n",
    "        self.conv1_1_s3 = nn.Conv2d(2*in_dim[2], in_dim[2], 1)\n",
    "        self.conv1_1_s4 = nn.Conv2d(2*in_dim[3], in_dim[3], 1)\n",
    "\n",
    "        # patch_embed\n",
    "        # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "        self.patch_embed1 = OverlapPatchEmbeddings(image_size, patch_sizes[0], strides[0], padding_sizes[0], 3, in_dim[0])\n",
    "        \n",
    "        self.patch_embed2_1 = OverlapPatchEmbeddings_fuse(image_size//4, patch_sizes1[1], strides[1], dil_padding_sizes1[1],in_dim[0], in_dim[1])\n",
    "        self.patch_embed2_2 = OverlapPatchEmbeddings_fuse(image_size//4, patch_sizes2[1], strides[1], dil_padding_sizes2[1],in_dim[0], in_dim[1])\n",
    "\n",
    "        self.patch_embed3_1 = OverlapPatchEmbeddings_fuse(image_size//8, patch_sizes1[2], strides[2], dil_padding_sizes1[2],in_dim[1], in_dim[2])\n",
    "        self.patch_embed3_2 = OverlapPatchEmbeddings_fuse(image_size//8, patch_sizes2[2], strides[2], dil_padding_sizes2[2],in_dim[1], in_dim[2])\n",
    "\n",
    "        self.patch_embed4_1 = OverlapPatchEmbeddings_fuse(image_size//16, patch_sizes1[3], strides[3], dil_padding_sizes1[3],in_dim[2], in_dim[3])\n",
    "        self.patch_embed4_2 = OverlapPatchEmbeddings_fuse(image_size//16, patch_sizes2[3], strides[3], dil_padding_sizes2[3],in_dim[2], in_dim[3])\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.block1 = nn.ModuleList([ \n",
    "            EfficientTransformerBlock(in_dim[0], key_dim[0], value_dim[0], head_count, token_mlp)\n",
    "        for _ in range(layers[0])])\n",
    "        self.norm1 = nn.LayerNorm(in_dim[0])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse(in_dim[1], key_dim[1], value_dim[1], head_count, token_mlp)\n",
    "        for _ in range(layers[1])])\n",
    "        self.norm2 = nn.LayerNorm(in_dim[1])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse(in_dim[2], key_dim[2], value_dim[2], head_count, token_mlp)\n",
    "        for _ in range(layers[2])])\n",
    "        self.norm3 = nn.LayerNorm(in_dim[2])\n",
    "\n",
    "        self.block4 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse(in_dim[3], key_dim[3], value_dim[3], head_count, token_mlp)\n",
    "        for _ in range(layers[3])])\n",
    "        self.norm4 = nn.LayerNorm(in_dim[3])\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        for blk in self.block1:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "      \n",
    "\n",
    "        # stage 2\n",
    "        # print(\"-------EN: Stage 2------\\n\\n\")\n",
    "        x1, H1, W1 = self.patch_embed2_1(x)\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2, H2, W2 = self.patch_embed2_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "        for blk in self.block2:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        nfx_cat = self.norm2(nfx_cat)\n",
    "        mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "        mx2 = nfx_cat[:, nfx1_len: :]\n",
    "        b, _, _ = mx1.shape\n",
    "        map_mx1 = mx1.reshape(b,H1,W1,-1)\n",
    "        map_mx2 = mx2.reshape(b,H2,W2,-1)\n",
    "        map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "        map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "        map_mx1 = F.interpolate(map_mx1,[self.Hs[1], self.Ws[1]])\n",
    "        cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "        x = self.conv1_1_s2(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "       \n",
    "        x1, H1, W1 = self.patch_embed3_1(x)\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2, H2, W2 = self.patch_embed3_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "        for blk in self.block3:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        nfx_cat = self.norm3(nfx_cat)\n",
    "        mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "        mx2 = nfx_cat[:, nfx1_len: :]\n",
    "        b, _, _ = mx1.shape\n",
    "        map_mx1 = mx1.reshape(b,H1,W1,-1)\n",
    "        map_mx2 = mx2.reshape(b,H2,W2,-1)\n",
    "        map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "        map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "        map_mx1 = F.interpolate(map_mx1,[self.Hs[2], self.Ws[2]])\n",
    "        cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "        x = self.conv1_1_s3(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "      \n",
    "        x1, H1, W1 = self.patch_embed4_1(x)\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2, H2, W2 = self.patch_embed4_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "        for blk in self.block4:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        nfx_cat = self.norm4(nfx_cat)\n",
    "        mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "        mx2 = nfx_cat[:, nfx1_len: :]\n",
    "        b, _, _ = mx1.shape\n",
    "        map_mx1 = mx1.reshape(b,H1,W1,-1)\n",
    "        map_mx2 = mx2.reshape(b,H2,W2,-1)\n",
    "        map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "        map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "        map_mx1 = F.interpolate(map_mx1,[self.Hs[3], self.Ws[3]])\n",
    "        cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "        x = self.conv1_1_s4(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9629443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_size, in_out_chan, head_count, token_mlp_mode, n_class=9,\n",
    "                 norm_layer=nn.LayerNorm, is_last=False):\n",
    "        super().__init__()\n",
    "        dims = in_out_chan[0]\n",
    "        out_dim = in_out_chan[1]\n",
    "        key_dim = in_out_chan[2]\n",
    "        value_dim = in_out_chan[3]\n",
    "        if not is_last:\n",
    "            self.concat_linear = nn.Linear(dims*2, out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.concat_linear = nn.Linear(dims*4, out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = FinalPatchExpand_X4(input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer)\n",
    "            # self.last_layer = nn.Linear(out_dim, n_class)\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class,1)\n",
    "            # self.last_layer = None\n",
    "\n",
    "        self.layer_former_1 = EfficientTransformerBlock(out_dim, key_dim, value_dim, head_count, token_mlp_mode)\n",
    "        self.layer_former_2 = EfficientTransformerBlock(out_dim, key_dim, value_dim, head_count, token_mlp_mode)\n",
    "       \n",
    "\n",
    "        def init_weights(self): \n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "      \n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:# skip connection exist\n",
    "            b, h, w, c = x2.shape\n",
    "            x2 = x2.view(b, -1, c)\n",
    "            cat_x = torch.cat([x1, x2], dim=-1)\n",
    "            cat_linear_x = self.concat_linear(cat_x)\n",
    "            tran_layer_1 = self.layer_former_1(cat_linear_x, h, w)\n",
    "            tran_layer_2 = self.layer_former_2(tran_layer_1, h, w)\n",
    "            \n",
    "            if self.last_layer:\n",
    "                out = self.last_layer(self.layer_up(tran_layer_2).view(b, 4*h, 4*w, -1).permute(0,3,1,2)) \n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)\n",
    "        else:\n",
    "            # if len(x1.shape)>3:\n",
    "            #     x1 = x1.permute(0,2,3,1)\n",
    "            #     b, h, w, c = x1.shape\n",
    "            #     x1 = x1.view(b, -1, c)\n",
    "            out = self.layer_up(x1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd03b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Transception(nn.Module):\n",
    "#     def __init__(self, num_classes=9, head_count=1, token_mlp_mode=\"mix_skip\"):\n",
    "#         super().__init__()\n",
    "    \n",
    "#         # Encoder\n",
    "#         dims, key_dim, value_dim, layers = [[64, 128, 320, 512], [64, 128, 320, 512], [64, 128, 320, 512], [2, 2, 2, 2]]        \n",
    "#         self.backbone = MiT(image_size=224, in_dim=dims, key_dim=key_dim, value_dim=value_dim, layers=layers,\n",
    "#                             head_count=head_count, token_mlp=token_mlp_mode)\n",
    "        \n",
    "#         # Decoder\n",
    "#         d_base_feat_size = 7 #16 for 512 input size, and 7 for 224\n",
    "#         in_out_chan = [[32, 64, 64, 64],[144, 128, 128, 128],[288, 320, 320, 320],[512, 512, 512, 512]]  # [dim, out_dim, key_dim, value_dim]\n",
    "\n",
    "#         self.decoder_3 = MyDecoderLayer((d_base_feat_size, d_base_feat_size), in_out_chan[3], head_count, \n",
    "#                                         token_mlp_mode, n_class=num_classes)\n",
    "#         self.decoder_2 = MyDecoderLayer((d_base_feat_size*2, d_base_feat_size*2), in_out_chan[2], head_count,\n",
    "#                                         token_mlp_mode, n_class=num_classes)\n",
    "#         self.decoder_1 = MyDecoderLayer((d_base_feat_size*4, d_base_feat_size*4), in_out_chan[1], head_count, \n",
    "#                                         token_mlp_mode, n_class=num_classes) \n",
    "#         self.decoder_0 = MyDecoderLayer((d_base_feat_size*8, d_base_feat_size*8), in_out_chan[0], head_count,\n",
    "#                                         token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         #---------------Encoder-------------------------\n",
    "#         if x.size()[1] == 1:\n",
    "#             x = x.repeat(1,3,1,1)\n",
    "\n",
    "#         output_enc = self.backbone(x)\n",
    "\n",
    "#         b,c,_,_ = output_enc[3].shape\n",
    "\n",
    "#         #---------------Decoder-------------------------     \n",
    "#         tmp_3 = self.decoder_3(output_enc[3].permute(0,2,3,1).view(b,-1,c))\n",
    "#         tmp_2 = self.decoder_2(tmp_3, output_enc[2].permute(0,2,3,1))\n",
    "#         tmp_1 = self.decoder_1(tmp_2, output_enc[1].permute(0,2,3,1))\n",
    "#         tmp_0 = self.decoder_0(tmp_1, output_enc[0].permute(0,2,3,1))\n",
    "\n",
    "#         return tmp_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa47a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transception(nn.Module):\n",
    "    def __init__(self, num_classes=9, head_count=1, token_mlp_mode=\"mix_skip\"):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Encoder\n",
    "        dims, key_dim, value_dim, layers = [[64, 128, 320, 512], [64, 128, 320, 512], [64, 128, 320, 512], [2, 2, 2, 2]]        \n",
    "        self.backbone = MiT_3inception(image_size=224, in_dim=dims, key_dim=key_dim, value_dim=value_dim, layers=layers,\n",
    "                            head_count=head_count, token_mlp=token_mlp_mode)\n",
    "        \n",
    "        # Decoder\n",
    "        d_base_feat_size = 7 #16 for 512 input size, and 7 for 224\n",
    "        in_out_chan = [[32, 64, 64, 64],[144, 128, 128, 128],[288, 320, 320, 320],[512, 512, 512, 512]]  # [dim, out_dim, key_dim, value_dim]\n",
    "\n",
    "        self.decoder_3 = MyDecoderLayer((d_base_feat_size, d_base_feat_size), in_out_chan[3], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_2 = MyDecoderLayer((d_base_feat_size*2, d_base_feat_size*2), in_out_chan[2], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_1 = MyDecoderLayer((d_base_feat_size*4, d_base_feat_size*4), in_out_chan[1], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes) \n",
    "        self.decoder_0 = MyDecoderLayer((d_base_feat_size*8, d_base_feat_size*8), in_out_chan[0], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #---------------Encoder-------------------------\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "\n",
    "        output_enc = self.backbone(x)\n",
    "\n",
    "        b,c,_,_ = output_enc[3].shape\n",
    "\n",
    "        #---------------Decoder-------------------------     \n",
    "        tmp_3 = self.decoder_3(output_enc[3].permute(0,2,3,1).view(b,-1,c))\n",
    "        tmp_2 = self.decoder_2(tmp_3, output_enc[2].permute(0,2,3,1))\n",
    "        tmp_1 = self.decoder_1(tmp_2, output_enc[1].permute(0,2,3,1))\n",
    "        tmp_0 = self.decoder_0(tmp_1, output_enc[0].permute(0,2,3,1))\n",
    "\n",
    "        return tmp_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0fb3b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 9, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "model = Transception(num_classes=9, head_count=1, token_mlp_mode=\"mix_skip\")\n",
    "print(model(torch.rand(24, 3, 224, 224)).shape)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac56f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd688c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
